*
* ==> Audit <==
* |--------------|-------------------------------------------------------------------------------------------|----------|----------|---------|---------------------|---------------------|
|   Command    |                                           Args                                            | Profile  |   User   | Version |     Start Time      |      End Time       |
|--------------|-------------------------------------------------------------------------------------------|----------|----------|---------|---------------------|---------------------|
| config       | set rootless true                                                                         | minikube | tobiasto | v1.28.0 | 01 Jan 23 18:41 GMT | 01 Jan 23 18:41 GMT |
| start        | --driver=docker                                                                           | minikube | tobiasto | v1.28.0 | 01 Jan 23 18:41 GMT |                     |
| config       | set rootless false                                                                        | minikube | tobiasto | v1.28.0 | 01 Jan 23 18:41 GMT | 01 Jan 23 18:41 GMT |
| start        | --driver=docker                                                                           | minikube | tobiasto | v1.28.0 | 01 Jan 23 18:41 GMT | 01 Jan 23 18:43 GMT |
| docker-env   |                                                                                           | minikube | tobiasto | v1.28.0 | 01 Jan 23 18:59 GMT | 01 Jan 23 18:59 GMT |
| stop         |                                                                                           | minikube | tobiasto | v1.28.0 | 01 Jan 23 19:04 GMT | 01 Jan 23 19:04 GMT |
| start        | --driver=docker                                                                           | minikube | tobiasto | v1.28.0 | 03 Jan 23 21:48 GMT |                     |
| start        | --driver=docker                                                                           | minikube | tobiasto | v1.28.0 | 03 Jan 23 21:48 GMT |                     |
| start        | --driver=docker                                                                           | minikube | tobiasto | v1.28.0 | 03 Jan 23 21:49 GMT | 03 Jan 23 21:49 GMT |
| stop         |                                                                                           | minikube | tobiasto | v1.28.0 | 03 Jan 23 23:17 GMT | 03 Jan 23 23:17 GMT |
| update-check |                                                                                           | minikube | tobiasto | v1.28.0 | 14 Jan 23 11:57 GMT | 14 Jan 23 11:57 GMT |
| update-check |                                                                                           | minikube | tobiasto | v1.28.0 | 08 Feb 23 22:25 GMT | 08 Feb 23 22:25 GMT |
| update-check |                                                                                           | minikube | tobiasto | v1.28.0 | 12 Mar 23 21:40 GMT | 12 Mar 23 21:40 GMT |
| update-check |                                                                                           | minikube | tobiasto | v1.28.0 | 12 Mar 23 21:44 GMT | 12 Mar 23 21:44 GMT |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 21:51 GMT |                     |
|              | --container-runtime=containerd                                                            |          |          |         |                     |                     |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 21:52 GMT |                     |
|              | --container-runtime=docker                                                                |          |          |         |                     |                     |
| delete       |                                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 21:52 GMT | 05 Jan 24 21:52 GMT |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 21:52 GMT | 05 Jan 24 21:53 GMT |
|              | --container-runtime=containerd                                                            |          |          |         |                     |                     |
| stop         |                                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 22:32 GMT | 05 Jan 24 22:32 GMT |
| config       | set rootless true                                                                         | minikube | tobiasto | v1.32.0 | 05 Jan 24 22:32 GMT | 05 Jan 24 22:32 GMT |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 22:32 GMT | 05 Jan 24 22:32 GMT |
|              | --container-runtime=containerd                                                            |          |          |         |                     |                     |
| stop         |                                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 22:33 GMT | 05 Jan 24 22:33 GMT |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 22:36 GMT | 05 Jan 24 22:36 GMT |
|              | --container-runtime=containerd                                                            |          |          |         |                     |                     |
| stop         |                                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:53 GMT | 05 Jan 24 23:53 GMT |
| start        | --driver=podman --container-runtime=containerd                                            | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:55 GMT |                     |
|              | --mount-string=/Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data |          |          |         |                     |                     |
|              | --mount                                                                                   |          |          |         |                     |                     |
| delete       |                                                                                           | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:55 GMT | 05 Jan 24 23:55 GMT |
| start        | --driver=podman --container-runtime=containerd                                            | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:56 GMT |                     |
|              | --mount-string=/Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data |          |          |         |                     |                     |
|              | --mount                                                                                   |          |          |         |                     |                     |
| config       | set rootless true                                                                         | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:56 GMT | 05 Jan 24 23:56 GMT |
| start        | --driver=podman --container-runtime=containerd                                            | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:56 GMT |                     |
|              | --mount-string=/Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data |          |          |         |                     |                     |
|              | --mount                                                                                   |          |          |         |                     |                     |
| start        | --driver=podman --container-runtime=containerd                                            | minikube | tobiasto | v1.32.0 | 05 Jan 24 23:58 GMT | 05 Jan 24 23:59 GMT |
|              | --mount-string=/Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data |          |          |         |                     |                     |
|              | --mount                                                                                   |          |          |         |                     |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data                | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:03 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data                | minikube | root     | v1.32.0 | 06 Jan 24 00:03 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/tmp                 | minikube | root     | v1.32.0 | 06 Jan 24 00:03 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/host                | minikube | root     | v1.32.0 | 06 Jan 24 00:04 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/home                | minikube | root     | v1.32.0 | 06 Jan 24 00:05 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/                    | minikube | root     | v1.32.0 | 06 Jan 24 00:05 GMT |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/mnt                 | minikube | root     | v1.32.0 | 06 Jan 24 00:07 GMT |                     |
| delete       |                                                                                           | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:08 GMT | 06 Jan 24 00:08 GMT |
| start        | --driver=podman --container-runtime=containerd                                            | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:08 GMT | 06 Jan 24 00:09 GMT |
|              | --mount-string=/Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data |          |          |         |                     |                     |
|              | --mount                                                                                   |          |          |         |                     |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data                | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:09 GMT |                     |
| delete       |                                                                                           | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:12 GMT | 06 Jan 24 00:12 GMT |
| start        | --driver=podman                                                                           | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:13 GMT | 06 Jan 24 00:13 GMT |
|              | --container-runtime=containerd                                                            |          |          |         |                     |                     |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data                | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:13 GMT |                     |
| config       | set rootless false                                                                        | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:13 GMT | 06 Jan 24 00:13 GMT |
| mount        | /Users/tobiasto/Projects/code-archives/logstash/k8s-cluster/logstash:/data                | minikube | tobiasto | v1.32.0 | 06 Jan 24 00:13 GMT |                     |
|--------------|-------------------------------------------------------------------------------------------|----------|----------|---------|---------------------|---------------------|

*
* ==> Last Start <==
* Log file created at: 2024/01/06 00:13:02
Running on machine: Cacbjiik-MBA
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0106 00:13:02.406654   95744 out.go:296] Setting OutFile to fd 1 ...
I0106 00:13:02.407376   95744 out.go:348] isatty.IsTerminal(1) = true
I0106 00:13:02.407378   95744 out.go:309] Setting ErrFile to fd 2...
I0106 00:13:02.407381   95744 out.go:348] isatty.IsTerminal(2) = true
I0106 00:13:02.407779   95744 root.go:338] Updating PATH: /Users/tobiasto/.minikube/bin
I0106 00:13:02.411038   95744 out.go:303] Setting JSON to false
I0106 00:13:02.434489   95744 start.go:128] hostinfo: {"hostname":"Cacbjiik-MBA.local","uptime":266722,"bootTime":1704233260,"procs":419,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.2.1","kernelVersion":"23.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"454c0807-6602-51d0-823d-cd2c1ff4e02e"}
W0106 00:13:02.434584   95744 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0106 00:13:02.440769   95744 out.go:177] üòÑ  minikube v1.32.0 on Darwin 14.2.1 (arm64)
I0106 00:13:02.456527   95744 out.go:177]     ‚ñ™ MINIKUBE_ROOTLESS=true
I0106 00:13:02.451191   95744 notify.go:220] Checking for updates...
I0106 00:13:02.460357   95744 driver.go:378] Setting default libvirt URI to qemu:///system
I0106 00:13:02.620059   95744 podman.go:123] podman version: 4.7.2
I0106 00:13:02.624688   95744 out.go:177] ‚ú®  Using the podman (experimental) driver based on user configuration
I0106 00:13:02.631466   95744 start.go:298] selected driver: podman
I0106 00:13:02.631603   95744 start.go:902] validating driver "podman" against <nil>
I0106 00:13:02.631612   95744 start.go:913] status for podman: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0106 00:13:02.632090   95744 cli_runner.go:164] Run: podman system info --format json
I0106 00:13:02.759866   95744 info.go:288] podman info: {Host:{BuildahVersion:1.32.0 CgroupVersion:v2 Conmon:{Package:conmon-2.1.8-2.fc39.aarch64 Path:/usr/bin/conmon Version:conmon version 2.1.8, commit: } Distribution:{Distribution:fedora Version:39} MemFree:1241788416 MemTotal:1979768832 OCIRuntime:{Name:crun Package:crun-1.12-1.fc39.aarch64 Path:/usr/bin/crun Version:crun version 1.12
commit: ce429cb2e277d001c2179df1ac66a470f00802ae
rundir: /run/user/501/crun
spec: 1.0.0
+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL} SwapFree:0 SwapTotal:0 Arch:arm64 Cpus:2 Eventlogger:journald Hostname:localhost.localdomain Kernel:6.6.3-200.fc39.aarch64 Os:linux Security:{Rootless:true} Uptime:0h 14m 23.00s} Registries:{Search:[docker.io]} Store:{ConfigFile:/var/home/core/.config/containers/storage.conf ContainerStore:{Number:0} GraphDriverName:overlay GraphOptions:{} GraphRoot:/var/home/core/.local/share/containers/storage GraphStatus:{BackingFilesystem:xfs NativeOverlayDiff:true SupportsDType:true UsingMetacopy:false} ImageStore:{Number:1} RunRoot:/run/user/501/containers VolumePath:/var/home/core/.local/share/containers/storage/volumes}}
I0106 00:13:02.759981   95744 start_flags.go:309] no existing cluster config was found, will generate one from the flags
I0106 00:13:02.764259   95744 out.go:177]
W0106 00:13:02.767750   95744 out.go:239] ‚õî  Requested memory allocation (1888MB) is less than the recommended minimum 1900MB. Deployments may fail.
I0106 00:13:02.774500   95744 out.go:177]
I0106 00:13:02.777555   95744 start_flags.go:394] Using suggested 1888MB memory alloc based on sys=8192MB, container=1888MB
I0106 00:13:02.777653   95744 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I0106 00:13:02.780592   95744 out.go:177] üìå  Using rootless Podman driver
I0106 00:13:02.783228   95744 cni.go:84] Creating CNI manager for ""
I0106 00:13:02.783389   95744 cni.go:143] "podman" driver + "containerd" runtime found, recommending kindnet
I0106 00:13:02.783392   95744 start_flags.go:318] Found "CNI" CNI - setting NetworkPlugin=cni
I0106 00:13:02.783396   95744 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:1888 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:containerd CRISocket: NetworkPlugin:cni FeatureGates:KubeletInUserNamespace=true ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0106 00:13:02.787597   95744 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0106 00:13:02.795930   95744 cache.go:121] Beginning downloading kic base image for podman with containerd
I0106 00:13:02.799789   95744 out.go:177] üöú  Pulling base image ...
I0106 00:13:02.806398   95744 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I0106 00:13:02.806511   95744 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime containerd
I0106 00:13:02.806528   95744 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I0106 00:13:02.806535   95744 preload.go:148] Found local preload: /Users/tobiasto/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-containerd-overlay2-arm64.tar.lz4
I0106 00:13:02.806536   95744 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I0106 00:13:02.806539   95744 cache.go:56] Caching tarball of preloaded images
I0106 00:13:02.806693   95744 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I0106 00:13:02.806699   95744 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I0106 00:13:02.806972   95744 preload.go:174] Found /Users/tobiasto/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-containerd-overlay2-arm64.tar.lz4 in cache, skipping download
I0106 00:13:02.806975   95744 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on containerd
I0106 00:13:02.807273   95744 profile.go:148] Saving config to /Users/tobiasto/.minikube/profiles/minikube/config.json ...
I0106 00:13:02.807282   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/config.json: {Name:mk1382a25c2f3f0052fade8ffa9dd1196e358460 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
E0106 00:13:02.807459   95744 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I0106 00:13:02.807466   95744 cache.go:194] Successfully downloaded all kic artifacts
I0106 00:13:02.808149   95744 start.go:365] acquiring machines lock for minikube: {Name:mk8bda06772a1cdcb842e49a557f3902e5275c73 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0106 00:13:02.808191   95744 start.go:369] acquired machines lock for "minikube" in 36.833¬µs
I0106 00:13:02.808335   95744 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:1888 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:containerd CRISocket: NetworkPlugin:cni FeatureGates:KubeletInUserNamespace=true ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:containerd ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:containerd ControlPlane:true Worker:true}
I0106 00:13:02.808368   95744 start.go:125] createHost starting for "" (driver="podman")
I0106 00:13:02.812413   95744 out.go:204] üî•  Creating podman container (CPUs=2, Memory=1888MB) ...
I0106 00:13:02.812826   95744 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I0106 00:13:02.813124   95744 client.go:168] LocalClient.Create starting
I0106 00:13:02.813331   95744 main.go:141] libmachine: Reading certificate data from /Users/tobiasto/.minikube/certs/ca.pem
I0106 00:13:02.813545   95744 main.go:141] libmachine: Decoding PEM data...
I0106 00:13:02.813552   95744 main.go:141] libmachine: Parsing certificate...
I0106 00:13:02.814098   95744 main.go:141] libmachine: Reading certificate data from /Users/tobiasto/.minikube/certs/cert.pem
I0106 00:13:02.814220   95744 main.go:141] libmachine: Decoding PEM data...
I0106 00:13:02.814226   95744 main.go:141] libmachine: Parsing certificate...
I0106 00:13:02.816705   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:02.938995   95744 cli_runner.go:164] Run: podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
W0106 00:13:03.038467   95744 cli_runner.go:211] podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}" returned with exit code 125
I0106 00:13:03.038564   95744 network_create.go:281] running [podman network inspect minikube] to gather additional debugging logs...
I0106 00:13:03.038575   95744 cli_runner.go:164] Run: podman network inspect minikube
W0106 00:13:03.132037   95744 cli_runner.go:211] podman network inspect minikube returned with exit code 125
I0106 00:13:03.132059   95744 network_create.go:284] error running [podman network inspect minikube]: podman network inspect minikube: exit status 125
stdout:
[]

stderr:
Error: network minikube: network not found
I0106 00:13:03.132066   95744 network_create.go:286] output of [podman network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr **
Error: network minikube: network not found

** /stderr **
I0106 00:13:03.132134   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:03.247952   95744 cli_runner.go:164] Run: podman network inspect podman --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I0106 00:13:03.331938   95744 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14002366880}
I0106 00:13:03.332166   95744 network_create.go:124] attempt to create podman network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 0 ...
I0106 00:13:03.332229   95744 cli_runner.go:164] Run: podman network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0106 00:13:03.424325   95744 network_create.go:108] podman network minikube 192.168.49.0/24 created
I0106 00:13:03.424366   95744 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0106 00:13:03.424448   95744 cli_runner.go:164] Run: podman ps -a --format {{.Names}}
I0106 00:13:03.515452   95744 cli_runner.go:164] Run: podman volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0106 00:13:03.604855   95744 oci.go:103] Successfully created a podman volume minikube
I0106 00:13:03.604966   95744 cli_runner.go:164] Run: podman run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I0106 00:13:04.224163   95744 oci.go:107] Successfully prepared a podman volume minikube
I0106 00:13:04.224204   95744 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime containerd
I0106 00:13:04.224220   95744 kic.go:194] Starting extracting preloaded images to volume ...
I0106 00:13:04.225211   95744 cli_runner.go:164] Run: podman run --rm --entrypoint /usr/bin/tar -v /Users/tobiasto/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-containerd-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I0106 00:13:07.961453   95744 cli_runner.go:217] Completed: podman run --rm --entrypoint /usr/bin/tar -v /Users/tobiasto/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-containerd-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (3.73177725s)
I0106 00:13:07.963316   95744 kic.go:203] duration metric: took 3.738893 seconds to extract preloaded images to volume
I0106 00:13:07.966387   95744 cli_runner.go:164] Run: podman info --format "'{{json .SecurityOptions}}'"
W0106 00:13:08.106170   95744 cli_runner.go:211] podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I0106 00:13:08.107002   95744 cli_runner.go:164] Run: podman run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var:exec --memory-swap=1888mb --memory=1888mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I0106 00:13:08.378397   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Running}}
I0106 00:13:08.481210   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:08.592397   95744 cli_runner.go:164] Run: podman exec minikube stat /var/lib/dpkg/alternatives/iptables
I0106 00:13:08.826910   95744 oci.go:144] the created container "minikube" has a running status.
I0106 00:13:08.827292   95744 kic.go:225] Creating ssh key for kic: /Users/tobiasto/.minikube/machines/minikube/id_rsa...
I0106 00:13:09.130162   95744 kic_runner.go:191] podman (temp): /Users/tobiasto/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0106 00:13:09.134897   95744 kic_runner.go:276] Run: /opt/homebrew/bin/podman exec -i minikube tee /home/docker/.ssh/authorized_keys
I0106 00:13:09.286205   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:09.383841   95744 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0106 00:13:09.383864   95744 kic_runner.go:114] Args: [podman exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0106 00:13:09.533572   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:09.622172   95744 machine.go:88] provisioning docker machine ...
I0106 00:13:09.623794   95744 ubuntu.go:169] provisioning hostname "minikube"
I0106 00:13:09.624528   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:09.742762   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:09.839699   95744 main.go:141] libmachine: Using SSH client type: native
I0106 00:13:09.844010   95744 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101186f80] 0x1011896f0 <nil>  [] 0s} 127.0.0.1 37217 <nil> <nil>}
I0106 00:13:09.844018   95744 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0106 00:13:09.959761   95744 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0106 00:13:09.960135   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:10.081199   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:10.178896   95744 main.go:141] libmachine: Using SSH client type: native
I0106 00:13:10.179187   95744 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101186f80] 0x1011896f0 <nil>  [] 0s} 127.0.0.1 37217 <nil> <nil>}
I0106 00:13:10.179194   95744 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts;
			fi
		fi
I0106 00:13:10.276301   95744 main.go:141] libmachine: SSH cmd err, output: <nil>:
I0106 00:13:10.276317   95744 ubuntu.go:175] set auth options {CertDir:/Users/tobiasto/.minikube CaCertPath:/Users/tobiasto/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/tobiasto/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/tobiasto/.minikube/machines/server.pem ServerKeyPath:/Users/tobiasto/.minikube/machines/server-key.pem ClientKeyPath:/Users/tobiasto/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/tobiasto/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/tobiasto/.minikube}
I0106 00:13:10.276331   95744 ubuntu.go:177] setting up certificates
I0106 00:13:10.276636   95744 provision.go:83] configureAuth start
I0106 00:13:10.276713   95744 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0106 00:13:10.377245   95744 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0106 00:13:10.472251   95744 provision.go:138] copyHostCerts
I0106 00:13:10.473086   95744 exec_runner.go:144] found /Users/tobiasto/.minikube/key.pem, removing ...
I0106 00:13:10.473095   95744 exec_runner.go:203] rm: /Users/tobiasto/.minikube/key.pem
I0106 00:13:10.473218   95744 exec_runner.go:151] cp: /Users/tobiasto/.minikube/certs/key.pem --> /Users/tobiasto/.minikube/key.pem (1675 bytes)
I0106 00:13:10.473534   95744 exec_runner.go:144] found /Users/tobiasto/.minikube/ca.pem, removing ...
I0106 00:13:10.473537   95744 exec_runner.go:203] rm: /Users/tobiasto/.minikube/ca.pem
I0106 00:13:10.473599   95744 exec_runner.go:151] cp: /Users/tobiasto/.minikube/certs/ca.pem --> /Users/tobiasto/.minikube/ca.pem (1082 bytes)
I0106 00:13:10.473872   95744 exec_runner.go:144] found /Users/tobiasto/.minikube/cert.pem, removing ...
I0106 00:13:10.473875   95744 exec_runner.go:203] rm: /Users/tobiasto/.minikube/cert.pem
I0106 00:13:10.473929   95744 exec_runner.go:151] cp: /Users/tobiasto/.minikube/certs/cert.pem --> /Users/tobiasto/.minikube/cert.pem (1127 bytes)
I0106 00:13:10.474074   95744 provision.go:112] generating server cert: /Users/tobiasto/.minikube/machines/server.pem ca-key=/Users/tobiasto/.minikube/certs/ca.pem private-key=/Users/tobiasto/.minikube/certs/ca-key.pem org=tobiasto.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0106 00:13:10.772417   95744 provision.go:172] copyRemoteCerts
I0106 00:13:10.772678   95744 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0106 00:13:10.772705   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:10.886697   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:10.982859   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:11.063231   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0106 00:13:11.076189   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0106 00:13:11.087356   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0106 00:13:11.097948   95744 provision.go:86] duration metric: configureAuth took 821.322708ms
I0106 00:13:11.098187   95744 ubuntu.go:193] setting minikube options for container-runtime
I0106 00:13:11.100096   95744 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=containerd, KubernetesVersion=v1.28.3
I0106 00:13:11.100119   95744 machine.go:91] provisioned docker machine in 1.477959208s
I0106 00:13:11.100122   95744 client.go:171] LocalClient.Create took 8.287168959s
I0106 00:13:11.100142   95744 start.go:167] duration metric: libmachine.API.Create for "minikube" took 8.287487708s
I0106 00:13:11.100145   95744 start.go:300] post-start starting for "minikube" (driver="podman")
I0106 00:13:11.100405   95744 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0106 00:13:11.100491   95744 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0106 00:13:11.100530   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:11.231435   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:11.320196   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:11.395763   95744 ssh_runner.go:195] Run: cat /etc/os-release
I0106 00:13:11.397842   95744 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0106 00:13:11.397859   95744 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0106 00:13:11.397868   95744 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0106 00:13:11.397873   95744 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0106 00:13:11.397882   95744 filesync.go:126] Scanning /Users/tobiasto/.minikube/addons for local assets ...
I0106 00:13:11.398101   95744 filesync.go:126] Scanning /Users/tobiasto/.minikube/files for local assets ...
I0106 00:13:11.398141   95744 start.go:303] post-start completed in 298.000708ms
I0106 00:13:11.398967   95744 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0106 00:13:11.493836   95744 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0106 00:13:11.586928   95744 profile.go:148] Saving config to /Users/tobiasto/.minikube/profiles/minikube/config.json ...
I0106 00:13:11.587682   95744 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0106 00:13:11.587712   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:11.704003   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:11.802270   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:11.879741   95744 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0106 00:13:11.882272   95744 start.go:128] duration metric: createHost completed in 9.074086875s
I0106 00:13:11.883084   95744 start.go:83] releasing machines lock for "minikube", held for 9.075076083s
I0106 00:13:11.883519   95744 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0106 00:13:11.973791   95744 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0106 00:13:12.064982   95744 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0106 00:13:12.065141   95744 ssh_runner.go:195] Run: cat /version.json
I0106 00:13:12.065176   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:12.065190   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:12.195721   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:12.200492   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:12.295506   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:12.299355   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:12.505989   95744 ssh_runner.go:195] Run: systemctl --version
I0106 00:13:12.509905   95744 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0106 00:13:12.512623   95744 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0106 00:13:12.525436   95744 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0106 00:13:12.525539   95744 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0106 00:13:12.537053   95744 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0106 00:13:12.537331   95744 start.go:472] detecting cgroup driver to use...
I0106 00:13:12.537341   95744 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0106 00:13:12.537814   95744 ssh_runner.go:195] Run: uname -r
I0106 00:13:12.539590   95744 ssh_runner.go:195] Run: sh -euc "(echo 6.6.3-200.fc39.aarch64; echo 5.11) | sort -V | head -n1"
I0106 00:13:12.541633   95744 ssh_runner.go:195] Run: uname -r
I0106 00:13:12.543266   95744 ssh_runner.go:195] Run: sh -euc "(echo 6.6.3-200.fc39.aarch64; echo 5.13) | sort -V | head -n1"
I0106 00:13:12.545071   95744 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0106 00:13:12.551922   95744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0106 00:13:12.557379   95744 docker.go:203] disabling cri-docker service (if available) ...
I0106 00:13:12.557647   95744 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.socket
I0106 00:13:12.565329   95744 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.service
I0106 00:13:12.572867   95744 ssh_runner.go:195] Run: sudo systemctl disable cri-docker.socket
I0106 00:13:12.624960   95744 ssh_runner.go:195] Run: sudo systemctl mask cri-docker.service
I0106 00:13:12.676943   95744 docker.go:219] disabling docker service ...
I0106 00:13:12.677020   95744 ssh_runner.go:195] Run: sudo systemctl stop -f docker.socket
I0106 00:13:12.687046   95744 ssh_runner.go:195] Run: sudo systemctl stop -f docker.service
I0106 00:13:12.693295   95744 ssh_runner.go:195] Run: sudo systemctl disable docker.socket
I0106 00:13:12.744518   95744 ssh_runner.go:195] Run: sudo systemctl mask docker.service
I0106 00:13:12.793706   95744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0106 00:13:12.802075   95744 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0106 00:13:12.810514   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0106 00:13:12.815313   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = true|' /etc/containerd/config.toml"
I0106 00:13:12.819817   95744 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0106 00:13:12.819896   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0106 00:13:12.824709   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0106 00:13:12.829331   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0106 00:13:12.833820   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0106 00:13:12.838258   95744 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0106 00:13:12.842655   95744 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0106 00:13:12.847250   95744 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0106 00:13:12.851936   95744 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0106 00:13:12.852167   95744 ssh_runner.go:195] Run: sudo modprobe br_netfilter
W0106 00:13:12.857631   95744 crio.go:151] "sudo sysctl net.bridge.bridge-nf-call-iptables" failed, which may be ok: sudo modprobe br_netfilter: Process exited with status 1
stdout:

stderr:
modprobe: ERROR: could not insert 'br_netfilter': Operation not permitted
I0106 00:13:12.857701   95744 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0106 00:13:12.862076   95744 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0106 00:13:12.911706   95744 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0106 00:13:12.977871   95744 start.go:519] Will wait 60s for socket path /run/containerd/containerd.sock
I0106 00:13:12.978429   95744 ssh_runner.go:195] Run: stat /run/containerd/containerd.sock
I0106 00:13:12.980703   95744 start.go:540] Will wait 60s for crictl version
I0106 00:13:12.980743   95744 ssh_runner.go:195] Run: which crictl
I0106 00:13:12.982448   95744 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0106 00:13:13.005761   95744 start.go:556] Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.24
RuntimeApiVersion:  v1
I0106 00:13:13.005888   95744 ssh_runner.go:195] Run: containerd --version
I0106 00:13:13.018872   95744 ssh_runner.go:195] Run: containerd --version
I0106 00:13:13.039371   95744 out.go:177] üì¶  Preparing Kubernetes v1.28.3 on containerd 1.6.24 ...
I0106 00:13:13.045355   95744 ssh_runner.go:195] Run: grep fe80::1	host.minikube.internal$ /etc/hosts
I0106 00:13:13.048201   95744 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "fe80::1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0106 00:13:13.055903   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:13.195089   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0106 00:13:13.286405   95744 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime containerd
I0106 00:13:13.286496   95744 ssh_runner.go:195] Run: sudo crictl images --output json
I0106 00:13:13.310241   95744 containerd.go:604] all images are preloaded for containerd runtime.
I0106 00:13:13.310608   95744 containerd.go:518] Images already preloaded, skipping extraction
I0106 00:13:13.310703   95744 ssh_runner.go:195] Run: sudo crictl images --output json
I0106 00:13:13.327601   95744 containerd.go:604] all images are preloaded for containerd runtime.
I0106 00:13:13.327610   95744 cache_images.go:84] Images are preloaded, skipping loading
I0106 00:13:13.327913   95744 ssh_runner.go:195] Run: sudo crictl info
I0106 00:13:13.346459   95744 cni.go:84] Creating CNI manager for ""
I0106 00:13:13.346464   95744 cni.go:143] "podman" driver + "containerd" runtime found, recommending kindnet
I0106 00:13:13.346991   95744 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0106 00:13:13.347002   95744 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/run/containerd/containerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota feature-gates:KubeletInUserNamespace=true] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true feature-gates:KubeletInUserNamespace=true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[feature-gates:KubeletInUserNamespace=true leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0106 00:13:13.347263   95744 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///run/containerd/containerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
    feature-gates: "KubeletInUserNamespace=true"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    feature-gates: "KubeletInUserNamespace=true"
    leader-elect: "false"
scheduler:
  extraArgs:
    feature-gates: "KubeletInUserNamespace=true"
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0106 00:13:13.347522   95744 kubeadm.go:976] kubelet [Unit]
Wants=containerd.service

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --feature-gates=KubeletInUserNamespace=true --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:containerd CRISocket: NetworkPlugin:cni FeatureGates:KubeletInUserNamespace=true ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0106 00:13:13.347598   95744 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0106 00:13:13.352493   95744 binaries.go:44] Found k8s binaries, skipping transfer
I0106 00:13:13.352561   95744 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0106 00:13:13.357079   95744 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (424 bytes)
I0106 00:13:13.365990   95744 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0106 00:13:13.374902   95744 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2244 bytes)
I0106 00:13:13.383801   95744 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0106 00:13:13.385754   95744 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0106 00:13:13.391673   95744 certs.go:56] Setting up /Users/tobiasto/.minikube/profiles/minikube for IP: 192.168.49.2
I0106 00:13:13.391693   95744 certs.go:190] acquiring lock for shared ca certs: {Name:mk09bbedc77b9a2ae7e42eb05fba23b8710cb0f3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.392733   95744 certs.go:199] skipping minikubeCA CA generation: /Users/tobiasto/.minikube/ca.key
I0106 00:13:13.392904   95744 certs.go:199] skipping proxyClientCA CA generation: /Users/tobiasto/.minikube/proxy-client-ca.key
I0106 00:13:13.393051   95744 certs.go:319] generating minikube-user signed cert: /Users/tobiasto/.minikube/profiles/minikube/client.key
I0106 00:13:13.393209   95744 crypto.go:68] Generating cert /Users/tobiasto/.minikube/profiles/minikube/client.crt with IP's: []
I0106 00:13:13.442967   95744 crypto.go:156] Writing cert to /Users/tobiasto/.minikube/profiles/minikube/client.crt ...
I0106 00:13:13.442982   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/client.crt: {Name:mkf17b65fb9d82fba847e162cbae7e901eca8350 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.443174   95744 crypto.go:164] Writing key to /Users/tobiasto/.minikube/profiles/minikube/client.key ...
I0106 00:13:13.443176   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/client.key: {Name:mkd8ba4428127f1977d38e8a6a7a8af82824345d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.443280   95744 certs.go:319] generating minikube signed cert: /Users/tobiasto/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0106 00:13:13.443286   95744 crypto.go:68] Generating cert /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0106 00:13:13.498123   95744 crypto.go:156] Writing cert to /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0106 00:13:13.498131   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mkd0a8fb7431b6ecc454d2568ffa8d75db9086cd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.498256   95744 crypto.go:164] Writing key to /Users/tobiasto/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0106 00:13:13.498258   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mkc21206de5c7b5e89dbcebc2ab97067d8c99626 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.498349   95744 certs.go:337] copying /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt
I0106 00:13:13.498453   95744 certs.go:341] copying /Users/tobiasto/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/tobiasto/.minikube/profiles/minikube/apiserver.key
I0106 00:13:13.498538   95744 certs.go:319] generating aggregator signed cert: /Users/tobiasto/.minikube/profiles/minikube/proxy-client.key
I0106 00:13:13.498543   95744 crypto.go:68] Generating cert /Users/tobiasto/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0106 00:13:13.676276   95744 crypto.go:156] Writing cert to /Users/tobiasto/.minikube/profiles/minikube/proxy-client.crt ...
I0106 00:13:13.676286   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/proxy-client.crt: {Name:mk1ba4e2910529386ff5338fcd4d93e21e5e4a8a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.676473   95744 crypto.go:164] Writing key to /Users/tobiasto/.minikube/profiles/minikube/proxy-client.key ...
I0106 00:13:13.676475   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.minikube/profiles/minikube/proxy-client.key: {Name:mk52b25e327d810c396aa6de0d176e3df8b4db8f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:13.676864   95744 certs.go:437] found cert: /Users/tobiasto/.minikube/certs/Users/tobiasto/.minikube/certs/ca-key.pem (1679 bytes)
I0106 00:13:13.676906   95744 certs.go:437] found cert: /Users/tobiasto/.minikube/certs/Users/tobiasto/.minikube/certs/ca.pem (1082 bytes)
I0106 00:13:13.676933   95744 certs.go:437] found cert: /Users/tobiasto/.minikube/certs/Users/tobiasto/.minikube/certs/cert.pem (1127 bytes)
I0106 00:13:13.676965   95744 certs.go:437] found cert: /Users/tobiasto/.minikube/certs/Users/tobiasto/.minikube/certs/key.pem (1675 bytes)
I0106 00:13:13.681384   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0106 00:13:13.693808   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0106 00:13:13.705287   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0106 00:13:13.716194   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0106 00:13:13.727217   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0106 00:13:13.737775   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0106 00:13:13.748470   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0106 00:13:13.759590   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0106 00:13:13.770889   95744 ssh_runner.go:362] scp /Users/tobiasto/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0106 00:13:13.782062   95744 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0106 00:13:13.790723   95744 ssh_runner.go:195] Run: openssl version
I0106 00:13:13.793770   95744 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0106 00:13:13.798523   95744 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0106 00:13:13.800330   95744 certs.go:480] hashing: -rw-r--r--. 1 root root 1111 Jan  1  2023 /usr/share/ca-certificates/minikubeCA.pem
I0106 00:13:13.800361   95744 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0106 00:13:13.803787   95744 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0106 00:13:13.808055   95744 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0106 00:13:13.810147   95744 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0106 00:13:13.810193   95744 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:1888 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:containerd CRISocket: NetworkPlugin:cni FeatureGates:KubeletInUserNamespace=true ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:containerd ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0106 00:13:13.810235   95744 cri.go:54] listing CRI containers in root /run/containerd/runc/k8s.io: {State:paused Name: Namespaces:[kube-system]}
I0106 00:13:13.810284   95744 ssh_runner.go:195] Run: sudo -s eval "crictl ps -a --quiet --label io.kubernetes.pod.namespace=kube-system"
I0106 00:13:13.827552   95744 cri.go:89] found id: ""
I0106 00:13:13.827891   95744 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0106 00:13:13.832453   95744 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0106 00:13:13.836643   95744 kubeadm.go:226] ignoring SystemVerification for kubeadm because of podman driver
I0106 00:13:13.836698   95744 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0106 00:13:13.841006   95744 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0106 00:13:13.841045   95744 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0106 00:13:13.877987   95744 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0106 00:13:13.878027   95744 kubeadm.go:322] [preflight] Running pre-flight checks
I0106 00:13:13.902802   95744 kubeadm.go:322] [preflight] The system verification failed. Printing the output from the verification:
I0106 00:13:13.902835   95744 kubeadm.go:322] [0;37mKERNEL_VERSION[0m: [0;32m6.6.3-200.fc39.aarch64[0m
I0106 00:13:13.902858   95744 kubeadm.go:322] [0;37mCONFIG_NAMESPACES[0m: [0;32menabled[0m
I0106 00:13:13.902884   95744 kubeadm.go:322] [0;37mCONFIG_NET_NS[0m: [0;32menabled[0m
I0106 00:13:13.902923   95744 kubeadm.go:322] [0;37mCONFIG_PID_NS[0m: [0;32menabled[0m
I0106 00:13:13.902960   95744 kubeadm.go:322] [0;37mCONFIG_IPC_NS[0m: [0;32menabled[0m
I0106 00:13:13.902983   95744 kubeadm.go:322] [0;37mCONFIG_UTS_NS[0m: [0;32menabled[0m
I0106 00:13:13.903009   95744 kubeadm.go:322] [0;37mCONFIG_CGROUPS[0m: [0;32menabled[0m
I0106 00:13:13.903036   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_CPUACCT[0m: [0;32menabled[0m
I0106 00:13:13.903066   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_DEVICE[0m: [0;32menabled[0m
I0106 00:13:13.903101   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_FREEZER[0m: [0;32menabled[0m
I0106 00:13:13.903134   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_PIDS[0m: [0;32menabled[0m
I0106 00:13:13.903157   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_SCHED[0m: [0;32menabled[0m
I0106 00:13:13.903182   95744 kubeadm.go:322] [0;37mCONFIG_CPUSETS[0m: [0;32menabled[0m
I0106 00:13:13.903204   95744 kubeadm.go:322] [0;37mCONFIG_MEMCG[0m: [0;32menabled[0m
I0106 00:13:13.903228   95744 kubeadm.go:322] [0;37mCONFIG_INET[0m: [0;32menabled[0m
I0106 00:13:13.903250   95744 kubeadm.go:322] [0;37mCONFIG_EXT4_FS[0m: [0;32menabled[0m
I0106 00:13:13.903270   95744 kubeadm.go:322] [0;37mCONFIG_PROC_FS[0m: [0;32menabled[0m
I0106 00:13:13.903311   95744 kubeadm.go:322] [0;37mCONFIG_NETFILTER_XT_TARGET_REDIRECT[0m: [0;32menabled (as module)[0m
I0106 00:13:13.903360   95744 kubeadm.go:322] [0;37mCONFIG_NETFILTER_XT_MATCH_COMMENT[0m: [0;32menabled (as module)[0m
I0106 00:13:13.903404   95744 kubeadm.go:322] [0;37mCONFIG_FAIR_GROUP_SCHED[0m: [0;32menabled[0m
I0106 00:13:13.903440   95744 kubeadm.go:322] [0;37mCONFIG_OVERLAY_FS[0m: [0;32menabled (as module)[0m
I0106 00:13:13.903472   95744 kubeadm.go:322] [0;37mCONFIG_AUFS_FS[0m: [0;33mnot set - Required for aufs.[0m
I0106 00:13:13.903497   95744 kubeadm.go:322] [0;37mCONFIG_BLK_DEV_DM[0m: [0;32menabled[0m
I0106 00:13:13.903533   95744 kubeadm.go:322] [0;37mCONFIG_CFS_BANDWIDTH[0m: [0;32menabled[0m
I0106 00:13:13.903572   95744 kubeadm.go:322] [0;37mCONFIG_CGROUP_HUGETLB[0m: [0;33mnot set - Required for hugetlb cgroup.[0m
I0106 00:13:13.903595   95744 kubeadm.go:322] [0;37mCONFIG_SECCOMP[0m: [0;32menabled[0m
I0106 00:13:13.903620   95744 kubeadm.go:322] [0;37mCONFIG_SECCOMP_FILTER[0m: [0;32menabled[0m
I0106 00:13:13.903639   95744 kubeadm.go:322] [0;37mOS[0m: [0;32mLinux[0m
I0106 00:13:13.903665   95744 kubeadm.go:322] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0106 00:13:13.903688   95744 kubeadm.go:322] [0;37mCGROUPS_CPUSET[0m: [0;31mmissing[0m
I0106 00:13:13.903715   95744 kubeadm.go:322] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0106 00:13:13.903741   95744 kubeadm.go:322] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0106 00:13:13.903766   95744 kubeadm.go:322] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0106 00:13:13.903788   95744 kubeadm.go:322] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0106 00:13:13.903811   95744 kubeadm.go:322] [0;37mCGROUPS_HUGETLB[0m: [0;33mmissing[0m
I0106 00:13:13.903830   95744 kubeadm.go:322] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0106 00:13:13.964453   95744 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0106 00:13:13.964503   95744 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0106 00:13:13.964557   95744 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0106 00:13:14.067833   95744 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0106 00:13:14.071769   95744 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0106 00:13:14.071819   95744 kubeadm.go:322] [certs] Using existing ca certificate authority
I0106 00:13:14.071849   95744 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0106 00:13:14.208436   95744 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0106 00:13:14.360418   95744 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0106 00:13:14.561234   95744 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0106 00:13:14.614162   95744 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0106 00:13:14.658274   95744 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0106 00:13:14.658625   95744 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0106 00:13:14.745491   95744 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0106 00:13:14.745639   95744 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0106 00:13:14.863425   95744 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0106 00:13:14.982139   95744 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0106 00:13:15.028515   95744 kubeadm.go:322] [certs] Generating "sa" key and public key
I0106 00:13:15.028557   95744 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0106 00:13:15.089552   95744 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0106 00:13:15.164378   95744 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0106 00:13:15.355529   95744 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0106 00:13:15.473220   95744 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0106 00:13:15.473573   95744 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0106 00:13:15.474922   95744 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0106 00:13:15.479267   95744 out.go:204]     ‚ñ™ Booting up control plane ...
I0106 00:13:15.479342   95744 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0106 00:13:15.479428   95744 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0106 00:13:15.479463   95744 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0106 00:13:15.485289   95744 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0106 00:13:15.485336   95744 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0106 00:13:15.485355   95744 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0106 00:13:15.544363   95744 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0106 00:13:19.546685   95744 kubeadm.go:322] [apiclient] All control plane components are healthy after 4.002060 seconds
I0106 00:13:19.546746   95744 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0106 00:13:19.554054   95744 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0106 00:13:20.064112   95744 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0106 00:13:20.064221   95744 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0106 00:13:20.571274   95744 kubeadm.go:322] [bootstrap-token] Using token: 6hr329.y9gxwpt632hoz890
I0106 00:13:20.578827   95744 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0106 00:13:20.578901   95744 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0106 00:13:20.580590   95744 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0106 00:13:20.588337   95744 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0106 00:13:20.589902   95744 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0106 00:13:20.592358   95744 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0106 00:13:20.593881   95744 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0106 00:13:20.600112   95744 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0106 00:13:20.703429   95744 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0106 00:13:20.983244   95744 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0106 00:13:20.985904   95744 kubeadm.go:322]
I0106 00:13:20.985937   95744 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0106 00:13:20.985939   95744 kubeadm.go:322]
I0106 00:13:20.986007   95744 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0106 00:13:20.986011   95744 kubeadm.go:322]
I0106 00:13:20.986024   95744 kubeadm.go:322]   mkdir -p $HOME/.kube
I0106 00:13:20.986073   95744 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0106 00:13:20.986095   95744 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0106 00:13:20.986097   95744 kubeadm.go:322]
I0106 00:13:20.986150   95744 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0106 00:13:20.986151   95744 kubeadm.go:322]
I0106 00:13:20.986183   95744 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0106 00:13:20.986186   95744 kubeadm.go:322]
I0106 00:13:20.986211   95744 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0106 00:13:20.986258   95744 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0106 00:13:20.986299   95744 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0106 00:13:20.986301   95744 kubeadm.go:322]
I0106 00:13:20.986346   95744 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0106 00:13:20.986392   95744 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0106 00:13:20.986393   95744 kubeadm.go:322]
I0106 00:13:20.986433   95744 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token 6hr329.y9gxwpt632hoz890 \
I0106 00:13:20.986486   95744 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:f0c0d4a5d87864eb4640db5ed977eada137f71aa2b1d486cdec5b5f458810954 \
I0106 00:13:20.986497   95744 kubeadm.go:322] 	--control-plane
I0106 00:13:20.986499   95744 kubeadm.go:322]
I0106 00:13:20.986540   95744 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0106 00:13:20.986543   95744 kubeadm.go:322]
I0106 00:13:20.986587   95744 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token 6hr329.y9gxwpt632hoz890 \
I0106 00:13:20.986637   95744 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:f0c0d4a5d87864eb4640db5ed977eada137f71aa2b1d486cdec5b5f458810954
I0106 00:13:20.988006   95744 kubeadm.go:322] 	[WARNING SystemVerification]: missing optional cgroups: hugetlb
I0106 00:13:20.988060   95744 kubeadm.go:322] 	[WARNING SystemVerification]: missing required cgroups: cpuset
I0106 00:13:20.988130   95744 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0106 00:13:20.988197   95744 kubeadm.go:322] 	[WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
I0106 00:13:20.988210   95744 cni.go:84] Creating CNI manager for ""
I0106 00:13:20.988219   95744 cni.go:143] "podman" driver + "containerd" runtime found, recommending kindnet
I0106 00:13:20.993048   95744 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I0106 00:13:21.001306   95744 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0106 00:13:21.005643   95744 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0106 00:13:21.005649   95744 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0106 00:13:21.016829   95744 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0106 00:13:21.472130   95744 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0106 00:13:21.472245   95744 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0106 00:13:21.472253   95744 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_01_06T00_13_21_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0106 00:13:21.479094   95744 ops.go:34] apiserver oom_adj: 3
I0106 00:13:21.479099   95744 ops.go:39] adjusting apiserver oom_adj to -10
I0106 00:13:21.479105   95744 ssh_runner.go:195] Run: /bin/bash -c "echo -10 | sudo tee /proc/$(pgrep kube-apiserver)/oom_adj"
I0106 00:13:21.529748   95744 kubeadm.go:1081] duration metric: took 57.586833ms to wait for elevateKubeSystemPrivileges.
W0106 00:13:21.529953   95744 kubeadm.go:292] unable to adjust resource limits: oom_adj adjust: /bin/bash -c "echo -10 | sudo tee /proc/$(pgrep kube-apiserver)/oom_adj": Process exited with status 1
stdout:
-10

stderr:
tee: /proc/1135/oom_adj: Permission denied
I0106 00:13:21.529967   95744 kubeadm.go:406] StartCluster complete in 7.719937583s
I0106 00:13:21.529978   95744 settings.go:142] acquiring lock: {Name:mk494d23dbaf7e4afe88d4dc6c95a0ba4ee32946 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:21.530074   95744 settings.go:150] Updating kubeconfig:  /Users/tobiasto/.kube/config
I0106 00:13:21.532213   95744 lock.go:35] WriteFile acquiring /Users/tobiasto/.kube/config: {Name:mk1c494307859875330f8b6687e267679cd88327 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0106 00:13:21.533741   95744 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=containerd, KubernetesVersion=v1.28.3
I0106 00:13:21.533765   95744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0106 00:13:21.533779   95744 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0106 00:13:21.534330   95744 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0106 00:13:21.534340   95744 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0106 00:13:21.534435   95744 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0106 00:13:21.534614   95744 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0106 00:13:21.535150   95744 host.go:66] Checking if "minikube" exists ...
I0106 00:13:21.535353   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:21.535367   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:21.578401   95744 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0106 00:13:21.578910   95744 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:containerd ControlPlane:true Worker:true}
I0106 00:13:21.585641   95744 out.go:177] üîé  Verifying Kubernetes components...
I0106 00:13:21.590058   95744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0106 00:13:21.605744   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:21.607694   95744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           fe80::1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0106 00:13:21.704360   95744 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0106 00:13:21.707654   95744 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0106 00:13:21.707672   95744 host.go:66] Checking if "minikube" exists ...
I0106 00:13:21.710976   95744 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0106 00:13:21.710984   95744 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0106 00:13:21.711036   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:21.711057   95744 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0106 00:13:21.840858   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0106 00:13:21.858274   95744 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0106 00:13:21.858334   95744 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0106 00:13:21.858405   95744 cli_runner.go:164] Run: podman version --format {{.Version}}
I0106 00:13:21.902390   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:21.909487   95744 start.go:926] {"host.minikube.internal": fe80::1} host record injected into CoreDNS's ConfigMap
I0106 00:13:21.957334   95744 api_server.go:52] waiting for apiserver process to appear ...
I0106 00:13:21.957410   95744 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0106 00:13:21.966498   95744 api_server.go:72] duration metric: took 387.573291ms to wait for apiserver process to appear ...
I0106 00:13:21.966515   95744 api_server.go:88] waiting for apiserver healthz status ...
I0106 00:13:21.966540   95744 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:35031/healthz ...
I0106 00:13:21.971810   95744 api_server.go:279] https://127.0.0.1:35031/healthz returned 200:
ok
I0106 00:13:21.976310   95744 api_server.go:141] control plane version: v1.28.3
I0106 00:13:21.976318   95744 api_server.go:131] duration metric: took 9.7995ms to wait for apiserver health ...
I0106 00:13:21.976322   95744 system_pods.go:43] waiting for kube-system pods to appear ...
I0106 00:13:21.996238   95744 system_pods.go:59] 4 kube-system pods found
I0106 00:13:21.996251   95744 system_pods.go:61] "etcd-minikube" [cf0e8f8e-6e87-4b90-9391-73e1f016351d] Pending
I0106 00:13:21.996254   95744 system_pods.go:61] "kube-apiserver-minikube" [bcf1d951-239b-4596-b5e3-c72193ff5189] Pending
I0106 00:13:21.996256   95744 system_pods.go:61] "kube-controller-manager-minikube" [6f5596d6-c600-4952-8d0d-dc84f83cd863] Pending
I0106 00:13:21.996257   95744 system_pods.go:61] "kube-scheduler-minikube" [0fb909ec-46c0-4754-9dc2-ffa2574e3944] Pending
I0106 00:13:21.996261   95744 system_pods.go:74] duration metric: took 19.936541ms to wait for pod list to return data ...
I0106 00:13:21.996265   95744 kubeadm.go:581] duration metric: took 417.350291ms to wait for : map[apiserver:true system_pods:true] ...
I0106 00:13:21.996272   95744 node_conditions.go:102] verifying NodePressure condition ...
I0106 00:13:21.998726   95744 node_conditions.go:122] node storage ephemeral capacity is 104266732Ki
I0106 00:13:21.998744   95744 node_conditions.go:123] node cpu capacity is 2
I0106 00:13:21.998749   95744 node_conditions.go:105] duration metric: took 2.475333ms to run NodePressure ...
I0106 00:13:21.998754   95744 start.go:228] waiting for startup goroutines ...
I0106 00:13:22.020188   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:22.045959   95744 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0106 00:13:22.112177   95744 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0106 00:13:22.139992   95744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37217 SSHKeyPath:/Users/tobiasto/.minikube/machines/minikube/id_rsa Username:docker}
I0106 00:13:22.226484   95744 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0106 00:13:22.355241   95744 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0106 00:13:22.363175   95744 addons.go:502] enable addons completed in 829.572083ms: enabled=[storage-provisioner default-storageclass]
I0106 00:13:22.363193   95744 start.go:233] waiting for cluster config update ...
I0106 00:13:22.363199   95744 start.go:242] writing updated cluster config ...
I0106 00:13:22.363986   95744 ssh_runner.go:195] Run: rm -f paused
I0106 00:13:22.508721   95744 start.go:600] kubectl: 1.29.0, cluster: 1.28.3 (minor skew: 1)
I0106 00:13:22.512405   95744 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

*
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
491127ba65cb7       ba04bb24b9575       5 seconds ago       Running             storage-provisioner       1                   193bfcf47bce9       storage-provisioner
1655c1be9e9f5       04b4eaa3d3db8       36 seconds ago      Running             kindnet-cni               0                   c89a848457cd1       kindnet-mjk82
e941f8df38e21       a5dd5cdd6d3ef       36 seconds ago      Running             kube-proxy                0                   b062668634a56       kube-proxy-tl29l
6d358ee1e8a5f       ba04bb24b9575       36 seconds ago      Exited              storage-provisioner       0                   193bfcf47bce9       storage-provisioner
5b9c4ee92ccbd       9cdd6470f48c8       54 seconds ago      Running             etcd                      0                   0477e673f11d7       etcd-minikube
6d27a3d28645d       537e9a59ee2fd       54 seconds ago      Running             kube-apiserver            0                   6cb50c93f9ede       kube-apiserver-minikube
d36cf625111e1       8276439b4f237       54 seconds ago      Running             kube-controller-manager   0                   5b373bb129f9c       kube-controller-manager-minikube
2b8dcee2539cc       42a4e73724daa       54 seconds ago      Running             kube-scheduler            0                   a4d7305d6f5d0       kube-scheduler-minikube

*
* ==> containerd <==
* Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.531872363Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-minikube,Uid:8d6f130c052cdecfb342cac375a57a94,Namespace:kube-system,Attempt:0,} returns sandbox id \"6cb50c93f9ede64d65d3bda02efd43e5721239841f04e5a9aa3e5ab38bb74d0f\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.533622712Z" level=info msg="CreateContainer within sandbox \"0477e673f11d7ad94fc9710278236ce58e99f26d56b1531a8587ac21b89ed29d\" for container &ContainerMetadata{Name:etcd,Attempt:0,}"
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.533680835Z" level=info msg="CreateContainer within sandbox \"6cb50c93f9ede64d65d3bda02efd43e5721239841f04e5a9aa3e5ab38bb74d0f\" for container &ContainerMetadata{Name:kube-apiserver,Attempt:0,}"
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.540796435Z" level=info msg="CreateContainer within sandbox \"a4d7305d6f5d0df149b317963b56e6b7f648cec95a8eb10d14d4dd9c2abe9461\" for &ContainerMetadata{Name:kube-scheduler,Attempt:0,} returns container id \"2b8dcee2539ccf1533ca852660c8319a35daea99728baaa32667461f0df65834\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.541355956Z" level=info msg="StartContainer for \"2b8dcee2539ccf1533ca852660c8319a35daea99728baaa32667461f0df65834\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.544824780Z" level=info msg="CreateContainer within sandbox \"5b373bb129f9cfca273edd24e8fdf0b66feec21e243badb9a1958328000c028e\" for &ContainerMetadata{Name:kube-controller-manager,Attempt:0,} returns container id \"d36cf625111e107066a16e84a30cac7f9bb46a52b8a0c6cd9f2dea8400ff95fb\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.545446881Z" level=info msg="StartContainer for \"d36cf625111e107066a16e84a30cac7f9bb46a52b8a0c6cd9f2dea8400ff95fb\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.547846914Z" level=info msg="CreateContainer within sandbox \"6cb50c93f9ede64d65d3bda02efd43e5721239841f04e5a9aa3e5ab38bb74d0f\" for &ContainerMetadata{Name:kube-apiserver,Attempt:0,} returns container id \"6d27a3d28645d96ce7c4ff286740fe839337ad497a959fe098748660a584ca64\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.548290105Z" level=info msg="StartContainer for \"6d27a3d28645d96ce7c4ff286740fe839337ad497a959fe098748660a584ca64\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.553842308Z" level=info msg="CreateContainer within sandbox \"0477e673f11d7ad94fc9710278236ce58e99f26d56b1531a8587ac21b89ed29d\" for &ContainerMetadata{Name:etcd,Attempt:0,} returns container id \"5b9c4ee92ccbd164ea51599aced5fba0f98ca43e17aa875fc4086b605b0f360c\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.554434952Z" level=info msg="StartContainer for \"5b9c4ee92ccbd164ea51599aced5fba0f98ca43e17aa875fc4086b605b0f360c\""
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.656926795Z" level=info msg="StartContainer for \"d36cf625111e107066a16e84a30cac7f9bb46a52b8a0c6cd9f2dea8400ff95fb\" returns successfully"
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.656989001Z" level=info msg="StartContainer for \"2b8dcee2539ccf1533ca852660c8319a35daea99728baaa32667461f0df65834\" returns successfully"
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.657021749Z" level=info msg="StartContainer for \"6d27a3d28645d96ce7c4ff286740fe839337ad497a959fe098748660a584ca64\" returns successfully"
Jan 06 00:13:16 minikube containerd[624]: time="2024-01-06T00:13:16.702690156Z" level=info msg="StartContainer for \"5b9c4ee92ccbd164ea51599aced5fba0f98ca43e17aa875fc4086b605b0f360c\" returns successfully"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.723365374Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:storage-provisioner,Uid:277dd492-ba23-471a-b45b-72483d8d46ec,Namespace:kube-system,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.738518457Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.738562205Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.738567496Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.738725032Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6 pid=1353 runtime=io.containerd.runc.v2
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.783674884Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:storage-provisioner,Uid:277dd492-ba23-471a-b45b-72483d8d46ec,Namespace:kube-system,Attempt:0,} returns sandbox id \"193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.786088457Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-tl29l,Uid:dde98a62-eb63-4245-9314-762410e0e22f,Namespace:kube-system,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.786482442Z" level=info msg="CreateContainer within sandbox \"193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6\" for container &ContainerMetadata{Name:storage-provisioner,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.793874449Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.793904406Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.793910656Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.793994153Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/b062668634a56b625da3390a7c784c750f5f67d4ef867090c1544f5ef4d3f195 pid=1395 runtime=io.containerd.runc.v2
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.807502966Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-mjk82,Uid:03a4dacf-9036-41f1-9aec-987733090250,Namespace:kube-system,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.818554290Z" level=info msg="CreateContainer within sandbox \"193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6\" for &ContainerMetadata{Name:storage-provisioner,Attempt:0,} returns container id \"6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.819892738Z" level=info msg="StartContainer for \"6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.822339602Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.822391642Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.822598384Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.823077407Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/c89a848457cd15bd62a19d82aaaadd860c06b2b4bc7f7e04741d16f6f75bd2ba pid=1420 runtime=io.containerd.runc.v2
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.829574532Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.846070521Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-tl29l,Uid:dde98a62-eb63-4245-9314-762410e0e22f,Namespace:kube-system,Attempt:0,} returns sandbox id \"b062668634a56b625da3390a7c784c750f5f67d4ef867090c1544f5ef4d3f195\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.848562134Z" level=info msg="CreateContainer within sandbox \"b062668634a56b625da3390a7c784c750f5f67d4ef867090c1544f5ef4d3f195\" for container &ContainerMetadata{Name:kube-proxy,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.866276493Z" level=info msg="CreateContainer within sandbox \"b062668634a56b625da3390a7c784c750f5f67d4ef867090c1544f5ef4d3f195\" for &ContainerMetadata{Name:kube-proxy,Attempt:0,} returns container id \"e941f8df38e218ddef010fb155eb0e68d5d937dd9b19d0771aa9f670dedc40bd\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.866701102Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-mjk82,Uid:03a4dacf-9036-41f1-9aec-987733090250,Namespace:kube-system,Attempt:0,} returns sandbox id \"c89a848457cd15bd62a19d82aaaadd860c06b2b4bc7f7e04741d16f6f75bd2ba\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.867076212Z" level=info msg="StartContainer for \"e941f8df38e218ddef010fb155eb0e68d5d937dd9b19d0771aa9f670dedc40bd\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.873162311Z" level=info msg="StartContainer for \"6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8\" returns successfully"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.874261769Z" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,} failed, error" error="failed to setup network for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\": failed to find network info for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.877125325Z" level=info msg="CreateContainer within sandbox \"c89a848457cd15bd62a19d82aaaadd860c06b2b4bc7f7e04741d16f6f75bd2ba\" for container &ContainerMetadata{Name:kindnet-cni,Attempt:0,}"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.891112619Z" level=info msg="CreateContainer within sandbox \"c89a848457cd15bd62a19d82aaaadd860c06b2b4bc7f7e04741d16f6f75bd2ba\" for &ContainerMetadata{Name:kindnet-cni,Attempt:0,} returns container id \"1655c1be9e9f5ee5be4ee33bbef315e23a9dd850287c166ebc14bd762c5c94bc\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.891583143Z" level=info msg="StartContainer for \"1655c1be9e9f5ee5be4ee33bbef315e23a9dd850287c166ebc14bd762c5c94bc\""
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.919628812Z" level=info msg="StartContainer for \"e941f8df38e218ddef010fb155eb0e68d5d937dd9b19d0771aa9f670dedc40bd\" returns successfully"
Jan 06 00:13:34 minikube containerd[624]: time="2024-01-06T00:13:34.923420041Z" level=info msg="StartContainer for \"1655c1be9e9f5ee5be4ee33bbef315e23a9dd850287c166ebc14bd762c5c94bc\" returns successfully"
Jan 06 00:13:41 minikube containerd[624]: time="2024-01-06T00:13:41.689803353Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Jan 06 00:13:47 minikube containerd[624]: time="2024-01-06T00:13:47.957844489Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,}"
Jan 06 00:13:48 minikube containerd[624]: time="2024-01-06T00:13:48.004245701Z" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,} failed, error" error="failed to setup network for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\": failed to find network info for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\""
Jan 06 00:14:00 minikube containerd[624]: time="2024-01-06T00:14:00.955579643Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,}"
Jan 06 00:14:00 minikube containerd[624]: time="2024-01-06T00:14:00.974295533Z" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-5dd5756b68-8k6ns,Uid:e345a1a7-4446-4ba6-b794-f84c525d110a,Namespace:kube-system,Attempt:0,} failed, error" error="failed to setup network for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\": failed to find network info for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\""
Jan 06 00:14:04 minikube containerd[624]: time="2024-01-06T00:14:04.942777132Z" level=info msg="shim disconnected" id=6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8
Jan 06 00:14:04 minikube containerd[624]: time="2024-01-06T00:14:04.942872962Z" level=warning msg="cleaning up after shim disconnected" id=6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8 namespace=k8s.io
Jan 06 00:14:04 minikube containerd[624]: time="2024-01-06T00:14:04.942881878Z" level=info msg="cleaning up dead shim"
Jan 06 00:14:04 minikube containerd[624]: time="2024-01-06T00:14:04.951180256Z" level=warning msg="cleanup warnings time=\"2024-01-06T00:14:04Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=1820 runtime=io.containerd.runc.v2\n"
Jan 06 00:14:05 minikube containerd[624]: time="2024-01-06T00:14:05.054546992Z" level=info msg="CreateContainer within sandbox \"193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6\" for container &ContainerMetadata{Name:storage-provisioner,Attempt:1,}"
Jan 06 00:14:05 minikube containerd[624]: time="2024-01-06T00:14:05.064019454Z" level=info msg="CreateContainer within sandbox \"193bfcf47bce9817b0db7d99ca336275e92bd4d0a6cd51870038d48f1560b8f6\" for &ContainerMetadata{Name:storage-provisioner,Attempt:1,} returns container id \"491127ba65cb7ac8ea2d0ac08a0d6fa268c0498c6d23cb59ef1f4d8398aaf49a\""
Jan 06 00:14:05 minikube containerd[624]: time="2024-01-06T00:14:05.064827801Z" level=info msg="StartContainer for \"491127ba65cb7ac8ea2d0ac08a0d6fa268c0498c6d23cb59ef1f4d8398aaf49a\""
Jan 06 00:14:05 minikube containerd[624]: time="2024-01-06T00:14:05.098025521Z" level=info msg="StartContainer for \"491127ba65cb7ac8ea2d0ac08a0d6fa268c0498c6d23cb59ef1f4d8398aaf49a\" returns successfully"

*
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_06T00_13_21_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 06 Jan 2024 00:13:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 06 Jan 2024 00:14:01 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 06 Jan 2024 00:13:41 +0000   Sat, 06 Jan 2024 00:13:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 06 Jan 2024 00:13:41 +0000   Sat, 06 Jan 2024 00:13:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 06 Jan 2024 00:13:41 +0000   Sat, 06 Jan 2024 00:13:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 06 Jan 2024 00:13:41 +0000   Sat, 06 Jan 2024 00:13:18 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  104266732Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             1933368Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  104266732Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             1933368Ki
  pods:               110
System Info:
  Machine ID:                 7534e309ba354fe9b489dc4fd0fe0cd2
  System UUID:                7534e309ba354fe9b489dc4fd0fe0cd2
  Boot ID:                    5e0a38eb-1a88-47c3-9b09-8a4b14111783
  Kernel Version:             6.6.3-200.fc39.aarch64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  containerd://1.6.24
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5dd5756b68-8k6ns            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (9%!)(MISSING)     37s
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (5%!)(MISSING)       0 (0%!)(MISSING)         50s
  kube-system                 kindnet-mjk82                       100m (5%!)(MISSING)     100m (5%!)(MISSING)   50Mi (2%!)(MISSING)        50Mi (2%!)(MISSING)      37s
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         50s
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         50s
  kube-system                 kube-proxy-tl29l                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         37s
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         52s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         49s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%!)(MISSING)   100m (5%!)(MISSING)
  memory             220Mi (11%!)(MISSING)  220Mi (11%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 35s   kube-proxy
  Normal  Starting                 51s   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  50s   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  50s   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    50s   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     50s   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           37s   node-controller  Node minikube event: Registered Node minikube in Controller

*
* ==> dmesg <==
* [Jan 5 23:58] KASLR disabled due to lack of seed
[  +0.000000] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +4.411615] dbus-broker-lau[827]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Jan 5 23:59] overlayfs: failed to set xattr on upper
[  +0.000003] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.
[  +0.334810] overlayfs: failed to set xattr on upper
[  +0.000003] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.
[  +0.084411] overlayfs: failed to set xattr on upper
[  +0.000007] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.
[  +0.263623] overlayfs: failed to set xattr on upper
[  +0.000003] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[  +3.054125] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[Jan 6 00:09] overlayfs: failed to set xattr on upper
[  +0.000006] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[  +0.345886] overlayfs: failed to set xattr on upper
[  +0.000029] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[  +0.073761] overlayfs: failed to set xattr on upper
[  +0.000002] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[  +0.156888] overlayfs: failed to set xattr on upper
[  +0.000003] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[Jan 6 00:13] overlayfs: failed to set xattr on upper
[  +0.000003] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.
[  +0.339824] overlayfs: failed to set xattr on upper
[  +0.000006] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.
[  +0.070069] overlayfs: failed to set xattr on upper
[  +0.000002] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000001] overlayfs: ...falling back to uuid=null.
[  +0.091833] overlayfs: failed to set xattr on upper
[  +0.000004] overlayfs: ...falling back to redirect_dir=nofollow.
[  +0.000000] overlayfs: ...falling back to uuid=null.

*
* ==> etcd [5b9c4ee92ccbd164ea51599aced5fba0f98ca43e17aa875fc4086b605b0f360c] <==
* {"level":"warn","ts":"2024-01-06T00:13:16.725258Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-06T00:13:16.725344Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-01-06T00:13:16.725394Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-06T00:13:16.7254Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-06T00:13:16.725412Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-06T00:13:16.725728Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-06T00:13:16.725931Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"arm64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-06T00:13:16.729708Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.575987ms"}
{"level":"info","ts":"2024-01-06T00:13:16.735032Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-01-06T00:13:16.735164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-01-06T00:13:16.73521Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-01-06T00:13:16.735214Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-01-06T00:13:16.735218Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-01-06T00:13:16.735235Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-01-06T00:13:16.737742Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-06T00:13:16.754515Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-01-06T00:13:16.755477Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-06T00:13:16.756227Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-06T00:13:16.757511Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-06T00:13:16.766057Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-06T00:13:16.768306Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-06T00:13:16.768543Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-06T00:13:16.768577Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-06T00:13:16.768827Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-01-06T00:13:16.770023Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-06T00:13:16.770205Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-06T00:13:16.770278Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-06T00:13:16.773218Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-06T00:13:16.773296Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-06T00:13:17.536038Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-01-06T00:13:17.536086Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-01-06T00:13:17.536099Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-01-06T00:13:17.536175Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-01-06T00:13:17.536195Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-06T00:13:17.536204Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-01-06T00:13:17.53621Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-01-06T00:13:17.536989Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-06T00:13:17.537066Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-06T00:13:17.537083Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-06T00:13:17.537373Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-06T00:13:17.537429Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-06T00:13:17.537438Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-06T00:13:17.537552Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-06T00:13:17.537871Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-06T00:13:17.538266Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-01-06T00:13:17.538337Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-06T00:13:17.538343Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}

*
* ==> kernel <==
*  00:14:11 up 15 min,  0 users,  load average: 0.19, 0.32, 0.20
Linux minikube 6.6.3-200.fc39.aarch64 #1 SMP PREEMPT_DYNAMIC Tue Nov 28 19:35:46 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

*
* ==> kindnet [1655c1be9e9f5ee5be4ee33bbef315e23a9dd850287c166ebc14bd762c5c94bc] <==
* I0106 00:13:35.046491       1 main.go:102] connected to apiserver: https://10.96.0.1:443
I0106 00:13:35.046516       1 main.go:107] hostIP = 192.168.49.2
podIP = 192.168.49.2
I0106 00:13:35.046628       1 main.go:116] setting mtu 1500 for CNI
I0106 00:13:35.046634       1 main.go:146] kindnetd IP family: "ipv4"
I0106 00:13:35.046640       1 main.go:150] noMask IPv4 subnets: [10.244.0.0/16]
I0106 00:14:05.178404       1 main.go:191] Failed to get nodes, retrying after error: Get "https://10.96.0.1:443/api/v1/nodes": dial tcp 10.96.0.1:443: i/o timeout
I0106 00:14:05.186780       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0106 00:14:05.186800       1 main.go:227] handling current node

*
* ==> kube-apiserver [6d27a3d28645d96ce7c4ff286740fe839337ad497a959fe098748660a584ca64] <==
* I0106 00:13:18.158452       1 secure_serving.go:213] Serving securely on [::]:8443
I0106 00:13:18.158507       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0106 00:13:18.158586       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0106 00:13:18.158670       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0106 00:13:18.158800       1 available_controller.go:423] Starting AvailableConditionController
I0106 00:13:18.158872       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0106 00:13:18.158920       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0106 00:13:18.159088       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0106 00:13:18.159124       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0106 00:13:18.159150       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0106 00:13:18.159173       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0106 00:13:18.159206       1 aggregator.go:164] waiting for initial CRD sync...
I0106 00:13:18.159237       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0106 00:13:18.159247       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0106 00:13:18.159407       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0106 00:13:18.159761       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0106 00:13:18.160223       1 controller.go:116] Starting legacy_token_tracking_controller
I0106 00:13:18.169905       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0106 00:13:18.160235       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0106 00:13:18.160243       1 controller.go:78] Starting OpenAPI AggregationController
I0106 00:13:18.160383       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0106 00:13:18.170143       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0106 00:13:18.207949       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0106 00:13:18.208174       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0106 00:13:18.209981       1 controller.go:134] Starting OpenAPI controller
I0106 00:13:18.210017       1 controller.go:85] Starting OpenAPI V3 controller
I0106 00:13:18.210032       1 naming_controller.go:291] Starting NamingConditionController
I0106 00:13:18.210054       1 establishing_controller.go:76] Starting EstablishingController
I0106 00:13:18.210085       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0106 00:13:18.210101       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0106 00:13:18.210128       1 crd_finalizer.go:266] Starting CRDFinalizer
I0106 00:13:18.259674       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0106 00:13:18.259856       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0106 00:13:18.259903       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0106 00:13:18.260521       1 controller.go:624] quota admission added evaluator for: namespaces
I0106 00:13:18.267925       1 shared_informer.go:318] Caches are synced for node_authorizer
I0106 00:13:18.269931       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0106 00:13:18.269943       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0106 00:13:18.270004       1 shared_informer.go:318] Caches are synced for configmaps
I0106 00:13:18.270205       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0106 00:13:18.270218       1 aggregator.go:166] initial CRD sync complete...
I0106 00:13:18.270228       1 autoregister_controller.go:141] Starting autoregister controller
I0106 00:13:18.270231       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0106 00:13:18.270233       1 cache.go:39] Caches are synced for autoregister controller
I0106 00:13:18.301288       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0106 00:13:19.167086       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0106 00:13:19.168961       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0106 00:13:19.168973       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0106 00:13:19.318819       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0106 00:13:19.331707       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0106 00:13:19.364872       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0106 00:13:19.367356       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0106 00:13:19.367903       1 controller.go:624] quota admission added evaluator for: endpoints
I0106 00:13:19.369336       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0106 00:13:20.199140       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0106 00:13:20.701555       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0106 00:13:20.708521       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0106 00:13:20.715663       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0106 00:13:34.443783       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0106 00:13:34.473035       1 controller.go:624] quota admission added evaluator for: replicasets.apps

*
* ==> kube-controller-manager [d36cf625111e107066a16e84a30cac7f9bb46a52b8a0c6cd9f2dea8400ff95fb] <==
* I0106 00:13:34.359518       1 shared_informer.go:311] Waiting for caches to sync for node
I0106 00:13:34.369897       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0106 00:13:34.377988       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0106 00:13:34.385506       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0106 00:13:34.398771       1 shared_informer.go:318] Caches are synced for PV protection
I0106 00:13:34.398757       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0106 00:13:34.398788       1 shared_informer.go:318] Caches are synced for PVC protection
I0106 00:13:34.398796       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0106 00:13:34.398751       1 shared_informer.go:318] Caches are synced for persistent volume
I0106 00:13:34.398802       1 shared_informer.go:318] Caches are synced for taint
I0106 00:13:34.399209       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0106 00:13:34.399352       1 shared_informer.go:318] Caches are synced for GC
I0106 00:13:34.398822       1 shared_informer.go:318] Caches are synced for disruption
I0106 00:13:34.399407       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0106 00:13:34.399417       1 taint_manager.go:211] "Sending events to api server"
I0106 00:13:34.399432       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0106 00:13:34.398968       1 shared_informer.go:318] Caches are synced for service account
I0106 00:13:34.399588       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0106 00:13:34.399608       1 shared_informer.go:318] Caches are synced for TTL
I0106 00:13:34.399932       1 shared_informer.go:318] Caches are synced for stateful set
I0106 00:13:34.399989       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0106 00:13:34.400027       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0106 00:13:34.400046       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0106 00:13:34.400366       1 shared_informer.go:318] Caches are synced for cronjob
I0106 00:13:34.400479       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0106 00:13:34.407761       1 shared_informer.go:318] Caches are synced for attach detach
I0106 00:13:34.407967       1 shared_informer.go:318] Caches are synced for expand
I0106 00:13:34.409483       1 shared_informer.go:318] Caches are synced for ephemeral
I0106 00:13:34.411628       1 shared_informer.go:318] Caches are synced for ReplicationController
I0106 00:13:34.421701       1 shared_informer.go:318] Caches are synced for namespace
I0106 00:13:34.428140       1 shared_informer.go:318] Caches are synced for daemon sets
I0106 00:13:34.430046       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0106 00:13:34.433339       1 shared_informer.go:318] Caches are synced for crt configmap
I0106 00:13:34.448622       1 shared_informer.go:318] Caches are synced for TTL after finished
I0106 00:13:34.448799       1 shared_informer.go:318] Caches are synced for deployment
I0106 00:13:34.449937       1 shared_informer.go:318] Caches are synced for HPA
I0106 00:13:34.450108       1 shared_informer.go:318] Caches are synced for job
I0106 00:13:34.450170       1 shared_informer.go:318] Caches are synced for endpoint
I0106 00:13:34.450247       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0106 00:13:34.450116       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0106 00:13:34.451230       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0106 00:13:34.459825       1 shared_informer.go:318] Caches are synced for node
I0106 00:13:34.460041       1 range_allocator.go:174] "Sending events to api server"
I0106 00:13:34.460054       1 range_allocator.go:178] "Starting range CIDR allocator"
I0106 00:13:34.460057       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0106 00:13:34.460059       1 shared_informer.go:318] Caches are synced for cidrallocator
I0106 00:13:34.461991       1 event.go:307] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-mjk82"
I0106 00:13:34.463378       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-tl29l"
I0106 00:13:34.479730       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0106 00:13:34.493315       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0106 00:13:34.499822       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0106 00:13:34.522528       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-8k6ns"
I0106 00:13:34.534185       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="39.944877ms"
I0106 00:13:34.547505       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="13.28653ms"
I0106 00:13:34.547871       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="342.196¬µs"
I0106 00:13:34.572580       1 shared_informer.go:318] Caches are synced for resource quota
I0106 00:13:34.620818       1 shared_informer.go:318] Caches are synced for resource quota
I0106 00:13:34.985887       1 shared_informer.go:318] Caches are synced for garbage collector
I0106 00:13:35.000138       1 shared_informer.go:318] Caches are synced for garbage collector
I0106 00:13:35.000234       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

*
* ==> kube-proxy [e941f8df38e218ddef010fb155eb0e68d5d937dd9b19d0771aa9f670dedc40bd] <==
* I0106 00:13:34.953009       1 server_others.go:69] "Using iptables proxy"
I0106 00:13:34.960549       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0106 00:13:34.981129       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0106 00:13:34.981963       1 server_others.go:152] "Using iptables Proxier"
I0106 00:13:34.981980       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0106 00:13:34.981986       1 server_others.go:438] "Defaulting to no-op detect-local"
I0106 00:13:34.982030       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0106 00:13:34.982346       1 server.go:846] "Version info" version="v1.28.3"
I0106 00:13:34.982355       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0106 00:13:35.184173       1 config.go:188] "Starting service config controller"
I0106 00:13:35.184205       1 config.go:97] "Starting endpoint slice config controller"
I0106 00:13:35.184210       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0106 00:13:35.184210       1 shared_informer.go:311] Waiting for caches to sync for service config
I0106 00:13:35.184224       1 config.go:315] "Starting node config controller"
I0106 00:13:35.184236       1 shared_informer.go:311] Waiting for caches to sync for node config
I0106 00:13:35.284754       1 shared_informer.go:318] Caches are synced for node config
I0106 00:13:35.284758       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0106 00:13:35.284765       1 shared_informer.go:318] Caches are synced for service config

*
* ==> kube-scheduler [2b8dcee2539ccf1533ca852660c8319a35daea99728baaa32667461f0df65834] <==
* I0106 00:13:17.165063       1 serving.go:348] Generated self-signed cert in-memory
W0106 00:13:18.213038       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0106 00:13:18.213068       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0106 00:13:18.213095       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0106 00:13:18.213099       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0106 00:13:18.227953       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0106 00:13:18.228061       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0106 00:13:18.228955       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0106 00:13:18.229000       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0106 00:13:18.229501       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0106 00:13:18.229739       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
W0106 00:13:18.230208       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0106 00:13:18.230295       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0106 00:13:18.231816       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:18.231829       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:18.233449       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0106 00:13:18.233595       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0106 00:13:18.235553       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0106 00:13:18.235617       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0106 00:13:18.235725       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0106 00:13:18.235761       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0106 00:13:18.235731       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0106 00:13:18.233710       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:18.235779       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:18.233732       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0106 00:13:18.235786       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0106 00:13:18.233685       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:18.235792       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:18.235809       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0106 00:13:18.235812       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0106 00:13:18.235844       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0106 00:13:18.235856       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0106 00:13:18.235872       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0106 00:13:18.235875       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0106 00:13:18.235923       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0106 00:13:18.235932       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0106 00:13:18.235939       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0106 00:13:18.233612       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0106 00:13:18.236107       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0106 00:13:18.236627       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:18.236702       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:19.048430       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:19.048455       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:19.083255       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0106 00:13:19.083293       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0106 00:13:19.160749       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0106 00:13:19.160811       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0106 00:13:19.240104       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0106 00:13:19.240125       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0106 00:13:21.329203       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

*
* ==> kubelet <==
* Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.063984    1218 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Jan 06 00:13:21 minikube kubelet[1218]: E0106 00:13:21.067207    1218 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: E0106 00:13:21.207121    1218 container_manager_linux.go:509] "Failed to ensure process in container with oom score" err="failed to apply oom score -999 to PID 1218: write /proc/1218/oom_score_adj: permission denied"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252544    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/9aac5b5c8815def09a2ef9e37b89da55-etcd-data\") pod \"etcd-minikube\" (UID: \"9aac5b5c8815def09a2ef9e37b89da55\") " pod="kube-system/etcd-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252573    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f0083bb555f9d515e93f221fe5ace472-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"f0083bb555f9d515e93f221fe5ace472\") " pod="kube-system/kube-scheduler-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252586    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/9aac5b5c8815def09a2ef9e37b89da55-etcd-certs\") pod \"etcd-minikube\" (UID: \"9aac5b5c8815def09a2ef9e37b89da55\") " pod="kube-system/etcd-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252599    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/8d6f130c052cdecfb342cac375a57a94-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"8d6f130c052cdecfb342cac375a57a94\") " pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252610    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252623    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252633    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252664    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/8d6f130c052cdecfb342cac375a57a94-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"8d6f130c052cdecfb342cac375a57a94\") " pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252678    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252687    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252696    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8d6f130c052cdecfb342cac375a57a94-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8d6f130c052cdecfb342cac375a57a94\") " pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252707    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8d6f130c052cdecfb342cac375a57a94-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8d6f130c052cdecfb342cac375a57a94\") " pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252719    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8d6f130c052cdecfb342cac375a57a94-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8d6f130c052cdecfb342cac375a57a94\") " pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252729    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.252738    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/de830704f76c10f438dbd25e4ed22ba2-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"de830704f76c10f438dbd25e4ed22ba2\") " pod="kube-system/kube-controller-manager-minikube"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.938051    1218 apiserver.go:52] "Watching apiserver"
Jan 06 00:13:21 minikube kubelet[1218]: I0106 00:13:21.952283    1218 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Jan 06 00:13:22 minikube kubelet[1218]: E0106 00:13:21.999605    1218 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jan 06 00:13:22 minikube kubelet[1218]: E0106 00:13:22.007415    1218 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jan 06 00:13:22 minikube kubelet[1218]: I0106 00:13:22.021597    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.021551612 podCreationTimestamp="2024-01-06 00:13:21 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:22.021434408 +0000 UTC m=+1.332722530" watchObservedRunningTime="2024-01-06 00:13:22.021551612 +0000 UTC m=+1.332839734"
Jan 06 00:13:22 minikube kubelet[1218]: I0106 00:13:22.051040    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.051014602 podCreationTimestamp="2024-01-06 00:13:21 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:22.050611534 +0000 UTC m=+1.361899615" watchObservedRunningTime="2024-01-06 00:13:22.051014602 +0000 UTC m=+1.362302683"
Jan 06 00:13:22 minikube kubelet[1218]: I0106 00:13:22.051205    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.051195345 podCreationTimestamp="2024-01-06 00:13:21 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:22.037424417 +0000 UTC m=+1.348712539" watchObservedRunningTime="2024-01-06 00:13:22.051195345 +0000 UTC m=+1.362483467"
Jan 06 00:13:22 minikube kubelet[1218]: I0106 00:13:22.057585    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=3.057534184 podCreationTimestamp="2024-01-06 00:13:19 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:22.057446312 +0000 UTC m=+1.368734435" watchObservedRunningTime="2024-01-06 00:13:22.057534184 +0000 UTC m=+1.368822306"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.418019    1218 topology_manager.go:215] "Topology Admit Handler" podUID="277dd492-ba23-471a-b45b-72483d8d46ec" podNamespace="kube-system" podName="storage-provisioner"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.428277    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/277dd492-ba23-471a-b45b-72483d8d46ec-tmp\") pod \"storage-provisioner\" (UID: \"277dd492-ba23-471a-b45b-72483d8d46ec\") " pod="kube-system/storage-provisioner"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.428313    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pn7j6\" (UniqueName: \"kubernetes.io/projected/277dd492-ba23-471a-b45b-72483d8d46ec-kube-api-access-pn7j6\") pod \"storage-provisioner\" (UID: \"277dd492-ba23-471a-b45b-72483d8d46ec\") " pod="kube-system/storage-provisioner"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.483939    1218 topology_manager.go:215] "Topology Admit Handler" podUID="dde98a62-eb63-4245-9314-762410e0e22f" podNamespace="kube-system" podName="kube-proxy-tl29l"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.487208    1218 topology_manager.go:215] "Topology Admit Handler" podUID="03a4dacf-9036-41f1-9aec-987733090250" podNamespace="kube-system" podName="kindnet-mjk82"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.527032    1218 topology_manager.go:215] "Topology Admit Handler" podUID="e345a1a7-4446-4ba6-b794-f84c525d110a" podNamespace="kube-system" podName="coredns-5dd5756b68-8k6ns"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.528393    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/03a4dacf-9036-41f1-9aec-987733090250-xtables-lock\") pod \"kindnet-mjk82\" (UID: \"03a4dacf-9036-41f1-9aec-987733090250\") " pod="kube-system/kindnet-mjk82"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.529194    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/dde98a62-eb63-4245-9314-762410e0e22f-xtables-lock\") pod \"kube-proxy-tl29l\" (UID: \"dde98a62-eb63-4245-9314-762410e0e22f\") " pod="kube-system/kube-proxy-tl29l"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.529262    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/dde98a62-eb63-4245-9314-762410e0e22f-lib-modules\") pod \"kube-proxy-tl29l\" (UID: \"dde98a62-eb63-4245-9314-762410e0e22f\") " pod="kube-system/kube-proxy-tl29l"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.529304    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/03a4dacf-9036-41f1-9aec-987733090250-lib-modules\") pod \"kindnet-mjk82\" (UID: \"03a4dacf-9036-41f1-9aec-987733090250\") " pod="kube-system/kindnet-mjk82"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.530181    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/03a4dacf-9036-41f1-9aec-987733090250-cni-cfg\") pod \"kindnet-mjk82\" (UID: \"03a4dacf-9036-41f1-9aec-987733090250\") " pod="kube-system/kindnet-mjk82"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.530281    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/dde98a62-eb63-4245-9314-762410e0e22f-kube-proxy\") pod \"kube-proxy-tl29l\" (UID: \"dde98a62-eb63-4245-9314-762410e0e22f\") " pod="kube-system/kube-proxy-tl29l"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.530514    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qzf5j\" (UniqueName: \"kubernetes.io/projected/dde98a62-eb63-4245-9314-762410e0e22f-kube-api-access-qzf5j\") pod \"kube-proxy-tl29l\" (UID: \"dde98a62-eb63-4245-9314-762410e0e22f\") " pod="kube-system/kube-proxy-tl29l"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.530568    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7nnxz\" (UniqueName: \"kubernetes.io/projected/03a4dacf-9036-41f1-9aec-987733090250-kube-api-access-7nnxz\") pod \"kindnet-mjk82\" (UID: \"03a4dacf-9036-41f1-9aec-987733090250\") " pod="kube-system/kindnet-mjk82"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.630912    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/e345a1a7-4446-4ba6-b794-f84c525d110a-config-volume\") pod \"coredns-5dd5756b68-8k6ns\" (UID: \"e345a1a7-4446-4ba6-b794-f84c525d110a\") " pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:34 minikube kubelet[1218]: I0106 00:13:34.630971    1218 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-t5vf6\" (UniqueName: \"kubernetes.io/projected/e345a1a7-4446-4ba6-b794-f84c525d110a-kube-api-access-t5vf6\") pod \"coredns-5dd5756b68-8k6ns\" (UID: \"e345a1a7-4446-4ba6-b794-f84c525d110a\") " pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:34 minikube kubelet[1218]: E0106 00:13:34.875184    1218 remote_runtime.go:193] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\": failed to find network info for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\""
Jan 06 00:13:34 minikube kubelet[1218]: E0106 00:13:34.875245    1218 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\": failed to find network info for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:34 minikube kubelet[1218]: E0106 00:13:34.875257    1218 kuberuntime_manager.go:1166] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\": failed to find network info for sandbox \"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:34 minikube kubelet[1218]: E0106 00:13:34.875299    1218 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\\\": rpc error: code = Unknown desc = failed to setup network for sandbox \\\"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\\\": failed to find network info for sandbox \\\"9dab7d5a9dcc7f0eebdc488be09e92351afb4dab7b31c4f2c404fe10b4f359c8\\\"\"" pod="kube-system/coredns-5dd5756b68-8k6ns" podUID="e345a1a7-4446-4ba6-b794-f84c525d110a"
Jan 06 00:13:35 minikube kubelet[1218]: I0106 00:13:35.012875    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=13.012794515 podCreationTimestamp="2024-01-06 00:13:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:35.007927869 +0000 UTC m=+14.319215950" watchObservedRunningTime="2024-01-06 00:13:35.012794515 +0000 UTC m=+14.324082596"
Jan 06 00:13:35 minikube kubelet[1218]: I0106 00:13:35.019403    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-tl29l" podStartSLOduration=1.019376969 podCreationTimestamp="2024-01-06 00:13:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:35.013265455 +0000 UTC m=+14.324553536" watchObservedRunningTime="2024-01-06 00:13:35.019376969 +0000 UTC m=+14.330665050"
Jan 06 00:13:35 minikube kubelet[1218]: I0106 00:13:35.488207    1218 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kindnet-mjk82" podStartSLOduration=1.488168367 podCreationTimestamp="2024-01-06 00:13:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-06 00:13:35.020109774 +0000 UTC m=+14.331397855" watchObservedRunningTime="2024-01-06 00:13:35.488168367 +0000 UTC m=+14.799456448"
Jan 06 00:13:41 minikube kubelet[1218]: I0106 00:13:41.689241    1218 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 06 00:13:41 minikube kubelet[1218]: I0106 00:13:41.690017    1218 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 06 00:13:48 minikube kubelet[1218]: E0106 00:13:48.004541    1218 remote_runtime.go:193] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\": failed to find network info for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\""
Jan 06 00:13:48 minikube kubelet[1218]: E0106 00:13:48.004736    1218 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\": failed to find network info for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:48 minikube kubelet[1218]: E0106 00:13:48.004758    1218 kuberuntime_manager.go:1166] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\": failed to find network info for sandbox \"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:13:48 minikube kubelet[1218]: E0106 00:13:48.004883    1218 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\\\": rpc error: code = Unknown desc = failed to setup network for sandbox \\\"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\\\": failed to find network info for sandbox \\\"93384f8586d3bcfaa262006c9611199f59e64b5a8f465bb64e4a36aeae02d67e\\\"\"" pod="kube-system/coredns-5dd5756b68-8k6ns" podUID="e345a1a7-4446-4ba6-b794-f84c525d110a"
Jan 06 00:14:00 minikube kubelet[1218]: E0106 00:14:00.974848    1218 remote_runtime.go:193] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\": failed to find network info for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\""
Jan 06 00:14:00 minikube kubelet[1218]: E0106 00:14:00.974913    1218 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\": failed to find network info for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:14:00 minikube kubelet[1218]: E0106 00:14:00.974926    1218 kuberuntime_manager.go:1166] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to setup network for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\": failed to find network info for sandbox \"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\"" pod="kube-system/coredns-5dd5756b68-8k6ns"
Jan 06 00:14:00 minikube kubelet[1218]: E0106 00:14:00.974996    1218 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-5dd5756b68-8k6ns_kube-system(e345a1a7-4446-4ba6-b794-f84c525d110a)\\\": rpc error: code = Unknown desc = failed to setup network for sandbox \\\"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\\\": failed to find network info for sandbox \\\"0a3db4bad93f01c42b16a3072388d140bf116d49ef22d6ade398255bbfaa3c93\\\"\"" pod="kube-system/coredns-5dd5756b68-8k6ns" podUID="e345a1a7-4446-4ba6-b794-f84c525d110a"
Jan 06 00:14:05 minikube kubelet[1218]: I0106 00:14:05.050608    1218 scope.go:117] "RemoveContainer" containerID="6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8"

*
* ==> storage-provisioner [491127ba65cb7ac8ea2d0ac08a0d6fa268c0498c6d23cb59ef1f4d8398aaf49a] <==
* I0106 00:14:05.106376       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0106 00:14:05.112205       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0106 00:14:05.112322       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0106 00:14:05.118403       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0106 00:14:05.118588       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_4041d8a0-24fe-4b7e-b46d-8e06907d6f68!
I0106 00:14:05.119815       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cca491c4-ad86-4ca4-ab73-52f52ea8bd29", APIVersion:"v1", ResourceVersion:"389", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_4041d8a0-24fe-4b7e-b46d-8e06907d6f68 became leader
I0106 00:14:05.219424       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_4041d8a0-24fe-4b7e-b46d-8e06907d6f68!

*
* ==> storage-provisioner [6d358ee1e8a5fe7f6155904cda288a89bdb7f38b7934eae1e5ae2c1405cc33c8] <==
* I0106 00:13:34.902255       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0106 00:14:04.913584       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout
